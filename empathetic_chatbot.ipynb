{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d2980c",
   "metadata": {},
   "source": [
    "# Project 02: Empathetic Conversational Chatbot\n",
    "## Transformer with Multi-Head Attention (Built from Scratch)\n",
    "\n",
    "**Objective:** Build a Transformer encoder-decoder chatbot that generates empathetic agent replies given a situation, emotion, and customer utterance.\n",
    "\n",
    "**Key Requirements:**\n",
    "- No pretrained model weights - all weights randomly initialized\n",
    "- Train end-to-end from scratch\n",
    "- Track BLEU, ROUGE-L, chrF, and Perplexity metrics\n",
    "- Save all intermediate models and data as .pkl files\n",
    "\n",
    "**Project Structure:**\n",
    "1. Preprocessing (normalization, tokenization, vocabulary building)\n",
    "2. Input/Output (X/Y) Definition\n",
    "3. Model Architecture (Transformer from scratch)\n",
    "4. Training (with teacher forcing)\n",
    "5. Evaluation (automatic metrics + qualitative analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b660c7af",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaaab76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence  # For padding variable length sequences\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import re  # For text normalization\n",
    "import math\n",
    "import time\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(f\"üé≤ Random seed set to: {SEED}\")\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "# ROBUST PLATFORM DETECTION & PATH SETUP\n",
    "def detect_platform():\n",
    "    \"\"\"\n",
    "    Robustly detect if running on Kaggle or Local.\n",
    "    Prioritizes environment variables over directory existence.\n",
    "    \"\"\"\n",
    "    # Check for Kaggle environment variables (most reliable)\n",
    "    has_kaggle_env = (\n",
    "        os.getenv('KAGGLE_KERNEL_RUN_TYPE') is not None or\n",
    "        os.getenv('KAGGLE_WORKING_DIR') is not None\n",
    "    )\n",
    "    \n",
    "    # Check for Kaggle directory structure (secondary check)\n",
    "    has_kaggle_dirs = (\n",
    "        os.path.exists('/kaggle/working') and \n",
    "        os.path.exists('/kaggle/input')\n",
    "    )\n",
    "    \n",
    "    # Must have BOTH environment variables AND directory structure for Kaggle\n",
    "    is_kaggle = has_kaggle_env and has_kaggle_dirs\n",
    "    \n",
    "    return 'kaggle' if is_kaggle else 'local'\n",
    "\n",
    "PLATFORM = detect_platform()\n",
    "BASE_DIR = '/kaggle/working' if PLATFORM == 'kaggle' else '.'\n",
    "\n",
    "def get_platform_path(relative_path):\n",
    "    \"\"\"Get platform-aware absolute path.\"\"\"\n",
    "    return os.path.join(BASE_DIR, relative_path)\n",
    "\n",
    "# Create necessary directories\n",
    "for directory in ['saved_models', 'saved_data', 'saved_vocab']:\n",
    "    os.makedirs(get_platform_path(directory), exist_ok=True)\n",
    "\n",
    "print(f\"\\nüåç Platform: {PLATFORM.upper()}\")\n",
    "print(f\"üìÅ Base directory: {os.path.abspath(BASE_DIR)}\")\n",
    "print(f\"‚úÖ Directories created: saved_models, saved_data, saved_vocab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70820ca0",
   "metadata": {},
   "source": [
    "## Section 2: Dataset Loading and Exploration\n",
    "\n",
    "Load the empathetic dialogue dataset and understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1bef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLATFORM-AWARE DATASET LOADING\n",
    "print(\"üîç Loading dataset...\")\n",
    "\n",
    "def find_dataset():\n",
    "    \"\"\"Find the dataset file based on platform.\"\"\"\n",
    "    if PLATFORM == 'kaggle':\n",
    "        # Search in Kaggle input directories\n",
    "        search_paths = []\n",
    "        if os.path.exists('/kaggle/input'):\n",
    "            for root, dirs, files in os.walk('/kaggle/input'):\n",
    "                for file in files:\n",
    "                    if 'emotion' in file.lower() and file.endswith('.csv'):\n",
    "                        return os.path.join(root, file)\n",
    "        # Common Kaggle paths\n",
    "        common_paths = [\n",
    "            '/kaggle/input/empathetic-dialogues/emotion-emotion_69k.csv',\n",
    "            '/kaggle/input/empathetic-dialogues-dataset/emotion-emotion_69k.csv',\n",
    "            '/kaggle/input/emotion-emotion_69k.csv',\n",
    "        ]\n",
    "        for path in common_paths:\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "    else:\n",
    "        # Local paths - search in current directory and common locations\n",
    "        local_paths = [\n",
    "            'emotion-emotion_69k.csv',  # Current directory (no prefix)\n",
    "            os.path.join(os.getcwd(), 'emotion-emotion_69k.csv'),  # Absolute path\n",
    "            os.path.abspath('emotion-emotion_69k.csv'),  # Absolute resolve\n",
    "            os.path.join('..', 'emotion-emotion_69k.csv'),  # Parent directory\n",
    "            os.path.join('data', 'emotion-emotion_69k.csv'),  # data folder\n",
    "        ]\n",
    "        print(f\"üîç Searching for dataset in local paths:\")\n",
    "        for path in local_paths:\n",
    "            print(f\"   Checking: {path}\")\n",
    "            if os.path.exists(path):\n",
    "                print(f\"   ‚úÖ Found!\")\n",
    "                return path\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Find and load dataset\n",
    "dataset_path = find_dataset()\n",
    "\n",
    "if dataset_path is None:\n",
    "    error_msg = f\"‚ùå Dataset 'emotion-emotion_69k.csv' not found!\\n\"\n",
    "    if PLATFORM == 'kaggle':\n",
    "        error_msg += \"üí° Please add the dataset to your Kaggle notebook inputs\"\n",
    "    else:\n",
    "        error_msg += f\"üí° Please place 'emotion-emotion_69k.csv' in: {os.getcwd()}\"\n",
    "    raise FileNotFoundError(error_msg)\n",
    "\n",
    "print(f\"\\n‚úÖ Found dataset at: {dataset_path}\")\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "print(f\"\\nüìä Dataset loaded successfully!\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Columns: {list(df.columns)}\")\n",
    "print(f\"\\nüëÄ First few rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c90090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the structure of the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"\\nColumn details:\")\n",
    "print(df.info())\n",
    "\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(f\"\\nUnique emotions:\")\n",
    "print(df['emotion'].unique())\n",
    "print(f\"\\nTotal unique emotions: {df['emotion'].nunique()}\")\n",
    "\n",
    "print(f\"\\nEmotion distribution:\")\n",
    "print(df['emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b582a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the empathetic_dialogues column structure\n",
    "print(\"Sample empathetic_dialogues entries:\\n\")\n",
    "for i in range(3):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Situation: {df.iloc[i]['Situation']}\")\n",
    "    print(f\"Emotion: {df.iloc[i]['emotion']}\")\n",
    "    print(f\"Dialogue: {df.iloc[i]['empathetic_dialogues']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fc214c",
   "metadata": {},
   "source": [
    "## Section 3: Text Preprocessing and Dialogue Parsing\n",
    "\n",
    "Parse the empathetic_dialogues column to extract Customer utterances and Agent replies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44fed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dialogue(dialogue_text, labels_text):\n",
    "    \"\"\"\n",
    "    Parse the empathetic_dialogues and labels columns to extract customer utterance and agent reply.\n",
    "    \n",
    "    Format: \n",
    "    - empathetic_dialogues: \"Customer :{customer_text}\\nAgent :\"\n",
    "    - labels: \"{agent_text}\"\n",
    "    \"\"\"\n",
    "    if pd.isna(dialogue_text) or pd.isna(labels_text):\n",
    "        return None, None\n",
    "    \n",
    "    # Extract customer utterance from empathetic_dialogues\n",
    "    if \"Customer :\" in dialogue_text:\n",
    "        # Split by \"Customer :\" and take the part after it\n",
    "        customer_part = dialogue_text.split(\"Customer :\")[1]\n",
    "        # Remove the \"Agent :\" part if it exists\n",
    "        if \"Agent :\" in customer_part:\n",
    "            customer_utterance = customer_part.split(\"Agent :\")[0].strip()\n",
    "        else:\n",
    "            customer_utterance = customer_part.strip()\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "    # Agent reply comes from the labels column\n",
    "    agent_reply = str(labels_text).strip()\n",
    "    \n",
    "    return customer_utterance, agent_reply\n",
    "\n",
    "\n",
    "# Test the parsing function\n",
    "print(\"Testing dialogue parsing:\\n\")\n",
    "for i in range(5):\n",
    "    dialogue = df.iloc[i]['empathetic_dialogues']\n",
    "    labels = df.iloc[i]['labels']\n",
    "    customer, agent = parse_dialogue(dialogue, labels)\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Customer: {customer}\")\n",
    "    print(f\"Agent: {agent}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2f81a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all dialogues and create structured dataset\n",
    "parsed_data = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    customer_utterance, agent_reply = parse_dialogue(row['empathetic_dialogues'], row['labels'])\n",
    "    \n",
    "    if customer_utterance and agent_reply:\n",
    "        parsed_data.append({\n",
    "            'situation': row['Situation'],\n",
    "            'emotion': row['emotion'],\n",
    "            'customer_utterance': customer_utterance,\n",
    "            'agent_reply': agent_reply\n",
    "        })\n",
    "\n",
    "# Create new dataframe\n",
    "df_parsed = pd.DataFrame(parsed_data)\n",
    "print(f\"Parsed dataset shape: {df_parsed.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df_parsed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb90146",
   "metadata": {},
   "source": [
    "## Section 4: Text Normalization Functions\n",
    "\n",
    "Implement text preprocessing: lowercase, clean whitespace, normalize punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e208f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text by:\n",
    "    1. Converting to lowercase\n",
    "    2. Cleaning whitespace\n",
    "    3. Normalizing punctuation\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Normalize whitespace (multiple spaces to single space)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Add space before and after punctuation for proper tokenization\n",
    "    text = re.sub(r'([.,!?;:])', r' \\1 ', text)\n",
    "    \n",
    "    # Remove extra spaces created by punctuation normalization\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Test normalization\n",
    "test_texts = [\n",
    "    \"I remember going to see the fireworks with my best friend.\",\n",
    "    \"it feels like hitting to blank wall when i see the darkness\",\n",
    "    \"Oh ya? I don't really see how\"\n",
    "]\n",
    "\n",
    "print(\"Testing text normalization:\\n\")\n",
    "for text in test_texts:\n",
    "    normalized = normalize_text(text)\n",
    "    print(f\"Original : {text}\")\n",
    "    print(f\"Normalized: {normalized}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb50157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply normalization to all text fields\n",
    "df_parsed['situation_normalized'] = df_parsed['situation'].apply(normalize_text)\n",
    "df_parsed['customer_utterance_normalized'] = df_parsed['customer_utterance'].apply(normalize_text)\n",
    "df_parsed['agent_reply_normalized'] = df_parsed['agent_reply'].apply(normalize_text)\n",
    "\n",
    "print(\"Normalized dataset sample:\")\n",
    "print(df_parsed[['emotion', 'situation_normalized', 'customer_utterance_normalized', 'agent_reply_normalized']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c035dfe",
   "metadata": {},
   "source": [
    "## Section 5: Dataset Splitting (Train 80%, Val 10%, Test 10%)\n",
    "\n",
    "Split the dataset with fixed random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b10784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split dataset\n",
    "df_shuffled = df_parsed.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# Calculate split sizes\n",
    "total_size = len(df_shuffled)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# Split data\n",
    "train_df = df_shuffled[:train_size]\n",
    "val_df = df_shuffled[train_size:train_size + val_size]\n",
    "test_df = df_shuffled[train_size + val_size:]\n",
    "\n",
    "print(f\"Dataset split:\")\n",
    "print(f\"Total samples: {total_size}\")\n",
    "print(f\"Training samples: {len(train_df)} ({len(train_df)/total_size*100:.1f}%)\")\n",
    "print(f\"Validation samples: {len(val_df)} ({len(val_df)/total_size*100:.1f}%)\")\n",
    "print(f\"Test samples: {len(test_df)} ({len(test_df)/total_size*100:.1f}%)\")\n",
    "\n",
    "# Save splits to pickle files\n",
    "with open('saved_data/train_df.pkl', 'wb') as f:\n",
    "    pickle.dump(train_df, f)\n",
    "    \n",
    "with open('saved_data/val_df.pkl', 'wb') as f:\n",
    "    pickle.dump(val_df, f)\n",
    "    \n",
    "with open('saved_data/test_df.pkl', 'wb') as f:\n",
    "    pickle.dump(test_df, f)\n",
    "\n",
    "print(\"\\n‚úì Dataset splits saved to 'saved_data/' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6757c576",
   "metadata": {},
   "source": [
    "## Section 6: Tokenization and Vocabulary Building\n",
    "\n",
    "Build vocabulary from training split only with special tokens: `<pad>`, `<bos>`, `<eos>`, `<unk>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2308511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    Simple tokenizer that splits on whitespace.\n",
    "    Text should already be normalized.\n",
    "    \"\"\"\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    Vocabulary class for managing word-to-index and index-to-word mappings.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word_count = Counter()\n",
    "        \n",
    "        # Special tokens\n",
    "        self.PAD_TOKEN = '<pad>'\n",
    "        self.BOS_TOKEN = '<bos>'\n",
    "        self.EOS_TOKEN = '<eos>'\n",
    "        self.UNK_TOKEN = '<unk>'\n",
    "        \n",
    "        # Initialize with special tokens\n",
    "        self.add_word(self.PAD_TOKEN)  # Index 0\n",
    "        self.add_word(self.BOS_TOKEN)  # Index 1\n",
    "        self.add_word(self.EOS_TOKEN)  # Index 2\n",
    "        self.add_word(self.UNK_TOKEN)  # Index 3\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        \"\"\"Add a word to vocabulary.\"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            idx = len(self.word2idx)\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "        self.word_count[word] += 1\n",
    "        \n",
    "    def add_sentence(self, sentence):\n",
    "        \"\"\"Add all words in a sentence to vocabulary.\"\"\"\n",
    "        tokens = simple_tokenize(sentence)\n",
    "        for word in tokens:\n",
    "            self.add_word(word)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "    \n",
    "    def get_idx(self, word):\n",
    "        \"\"\"Get index of a word (returns UNK index if word not found).\"\"\"\n",
    "        return self.word2idx.get(word, self.word2idx[self.UNK_TOKEN])\n",
    "    \n",
    "    def get_word(self, idx):\n",
    "        \"\"\"Get word from index.\"\"\"\n",
    "        return self.idx2word.get(idx, self.UNK_TOKEN)\n",
    "\n",
    "\n",
    "print(\"Vocabulary class created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6486246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from TRAINING DATA ONLY\n",
    "vocab = Vocabulary()\n",
    "\n",
    "print(\"Building vocabulary from training data...\")\n",
    "\n",
    "# Add all text from training set to vocabulary\n",
    "for idx, row in train_df.iterrows():\n",
    "    vocab.add_sentence(row['situation_normalized'])\n",
    "    vocab.add_sentence(row['customer_utterance_normalized'])\n",
    "    vocab.add_sentence(row['agent_reply_normalized'])\n",
    "    vocab.add_word(row['emotion'])  # Add emotion as a word\n",
    "    \n",
    "print(f\"\\nVocabulary built successfully!\")\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"\\nSpecial token indices:\")\n",
    "print(f\"  PAD: {vocab.word2idx[vocab.PAD_TOKEN]}\")\n",
    "print(f\"  BOS: {vocab.word2idx[vocab.BOS_TOKEN]}\")\n",
    "print(f\"  EOS: {vocab.word2idx[vocab.EOS_TOKEN]}\")\n",
    "print(f\"  UNK: {vocab.word2idx[vocab.UNK_TOKEN]}\")\n",
    "\n",
    "# Show most common words\n",
    "print(f\"\\nTop 20 most common words:\")\n",
    "for word, count in vocab.word_count.most_common(20):\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dc3a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vocabulary to pickle file\n",
    "vocab_path = get_platform_path('saved_vocab/vocabulary.pkl')\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "print(f\"‚úì Vocabulary saved to '{vocab_path}'\")\n",
    "\n",
    "# Test vocabulary\n",
    "test_sentence = \"i remember going to the fireworks .\"\n",
    "tokens = simple_tokenize(test_sentence)\n",
    "indices = [vocab.get_idx(word) for word in tokens]\n",
    "\n",
    "print(f\"\\nTest tokenization:\")\n",
    "print(f\"Sentence: {test_sentence}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Indices: {indices}\")\n",
    "print(f\"Reconstructed: {[vocab.get_word(idx) for idx in indices]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8af0ee",
   "metadata": {},
   "source": [
    "## Section 7: Input/Output (X and Y) Definition\n",
    "\n",
    "Format inputs as: `\"Emotion: {emotion} | Situation: {situation} | Customer: {customer_utterance} Agent:\"`\n",
    "\n",
    "Format targets as: `\"{agent_reply}\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c89278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_output_pair(row):\n",
    "    \"\"\"\n",
    "    Create input (X) and output (Y) pair from a data row.\n",
    "    \n",
    "    Input format: \"Emotion: {emotion} | Situation: {situation} | Customer: {customer_utterance} Agent:\"\n",
    "    Output format: \"{agent_reply}\"\n",
    "    \"\"\"\n",
    "    emotion = row['emotion']\n",
    "    situation = row['situation_normalized']\n",
    "    customer = row['customer_utterance_normalized']\n",
    "    agent = row['agent_reply_normalized']\n",
    "    \n",
    "    # Create input string\n",
    "    input_text = f\"emotion : {emotion} | situation : {situation} | customer : {customer} agent :\"\n",
    "    \n",
    "    # Output is just the agent reply\n",
    "    output_text = agent\n",
    "    \n",
    "    return input_text, output_text\n",
    "\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing input/output creation:\\n\")\n",
    "for i in range(3):\n",
    "    row = train_df.iloc[i]\n",
    "    input_text, output_text = create_input_output_pair(row)\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Input (X): {input_text}\")\n",
    "    print(f\"Output (Y): {output_text}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c200985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequence(text, vocab, add_special_tokens=True):\n",
    "    \"\"\"\n",
    "    Encode a text sequence to indices using vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        vocab: Vocabulary object\n",
    "        add_special_tokens: If True, add <bos> and <eos> tokens\n",
    "    \n",
    "    Returns:\n",
    "        List of token indices\n",
    "    \"\"\"\n",
    "    tokens = simple_tokenize(text)\n",
    "    indices = [vocab.get_idx(token) for token in tokens]\n",
    "    \n",
    "    if add_special_tokens:\n",
    "        # Add BOS at beginning and EOS at end\n",
    "        indices = [vocab.word2idx[vocab.BOS_TOKEN]] + indices + [vocab.word2idx[vocab.EOS_TOKEN]]\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "# Test encoding\n",
    "test_input, test_output = create_input_output_pair(train_df.iloc[0])\n",
    "input_indices = encode_sequence(test_input, vocab, add_special_tokens=False)\n",
    "output_indices = encode_sequence(test_output, vocab, add_special_tokens=True)\n",
    "\n",
    "print(\"Testing sequence encoding:\\n\")\n",
    "print(f\"Input text: {test_input[:100]}...\")\n",
    "print(f\"Input length: {len(input_indices)} tokens\")\n",
    "print(f\"\\nOutput text: {test_output}\")\n",
    "print(f\"Output indices (with BOS/EOS): {output_indices}\")\n",
    "print(f\"Output length: {len(output_indices)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c83656",
   "metadata": {},
   "source": [
    "## Section 8: Transformer Architecture - Positional Encoding\n",
    "\n",
    "Implement sinusoidal positional encoding:\n",
    "- PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "- PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0967fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement sinusoidal positional encoding for Transformer.\n",
    "    \n",
    "    PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Compute the div_term for the encoding\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension\n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
    "        \n",
    "        # Register as buffer (not a parameter, but part of the model state)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input embeddings of shape (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            Positional encoded embeddings\n",
    "        \"\"\"\n",
    "        # Add positional encoding to input embeddings\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# Test positional encoding\n",
    "print(\"Testing Positional Encoding:\")\n",
    "d_model = 512\n",
    "pe = PositionalEncoding(d_model=d_model)\n",
    "\n",
    "# Create dummy input (batch_size=2, seq_len=10, d_model=512)\n",
    "dummy_input = torch.randn(2, 10, d_model)\n",
    "output = pe(dummy_input)\n",
    "\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"‚úì Positional Encoding implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfd55fd",
   "metadata": {},
   "source": [
    "## Section 9: Multi-Head Attention Mechanism\n",
    "\n",
    "Implement Multi-Head Attention from scratch with:\n",
    "- Query, Key, Value projections\n",
    "- Scaled dot-product attention\n",
    "- Multiple attention heads\n",
    "- Concatenation and final projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ba679",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention mechanism from scratch.\n",
    "    \n",
    "    Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Final output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Compute scaled dot-product attention.\n",
    "        \n",
    "        Args:\n",
    "            Q: Queries (batch_size, num_heads, seq_len, d_k)\n",
    "            K: Keys (batch_size, num_heads, seq_len, d_k)\n",
    "            V: Values (batch_size, num_heads, seq_len, d_k)\n",
    "            mask: Optional mask (batch_size, 1, seq_len, seq_len) or (batch_size, 1, 1, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            attention_output, attention_weights\n",
    "        \"\"\"\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return attention_output, attention_weights\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, d_k).\n",
    "        Transpose to get shape (batch_size, num_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        Combine heads back to original shape.\n",
    "        \"\"\"\n",
    "        batch_size, _, seq_len, _ = x.size()\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        return x.view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: Query tensor (batch_size, seq_len_q, d_model)\n",
    "            key: Key tensor (batch_size, seq_len_k, d_model)\n",
    "            value: Value tensor (batch_size, seq_len_v, d_model)\n",
    "            mask: Optional mask tensor\n",
    "        \n",
    "        Returns:\n",
    "            output, attention_weights\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(query)  # (batch_size, seq_len_q, d_model)\n",
    "        K = self.W_k(key)    # (batch_size, seq_len_k, d_model)\n",
    "        V = self.W_v(value)  # (batch_size, seq_len_v, d_model)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        Q = self.split_heads(Q)  # (batch_size, num_heads, seq_len_q, d_k)\n",
    "        K = self.split_heads(K)  # (batch_size, num_heads, seq_len_k, d_k)\n",
    "        V = self.split_heads(V)  # (batch_size, num_heads, seq_len_v, d_k)\n",
    "        \n",
    "        # Apply scaled dot-product attention\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads\n",
    "        attention_output = self.combine_heads(attention_output)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_o(attention_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "# Test Multi-Head Attention\n",
    "print(\"Testing Multi-Head Attention:\")\n",
    "d_model = 512\n",
    "num_heads = 2\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Create dummy inputs\n",
    "batch_size, seq_len = 2, 10\n",
    "dummy_query = torch.randn(batch_size, seq_len, d_model)\n",
    "dummy_key = torch.randn(batch_size, seq_len, d_model)\n",
    "dummy_value = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attn_weights = mha(dummy_query, dummy_key, dummy_value)\n",
    "\n",
    "print(f\"Query shape: {dummy_query.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(\"‚úì Multi-Head Attention implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568b1788",
   "metadata": {},
   "source": [
    "## Section 10: Position-wise Feed-Forward Network\n",
    "\n",
    "Implement FFN(x) = max(0, xW1 + b1)W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95efcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network.\n",
    "    \n",
    "    FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # First linear layer + ReLU\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second linear layer\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Test Feed-Forward Network\n",
    "print(\"Testing Position-wise Feed-Forward Network:\")\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "\n",
    "dummy_input = torch.randn(2, 10, d_model)\n",
    "output = ffn(dummy_input)\n",
    "\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"‚úì Feed-Forward Network implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36980678",
   "metadata": {},
   "source": [
    "## Section 11: Transformer Encoder Layer\n",
    "\n",
    "Build Encoder layer with:\n",
    "- Multi-Head Self-Attention\n",
    "- Add & Norm (Residual connection + Layer Normalization)\n",
    "- Feed-Forward Network\n",
    "- Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127beeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Transformer Encoder Layer.\n",
    "    \n",
    "    Components:\n",
    "    1. Multi-Head Self-Attention\n",
    "    2. Add & Norm (Residual + LayerNorm)\n",
    "    3. Feed-Forward Network\n",
    "    4. Add & Norm\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, d_model)\n",
    "            mask: Optional mask for padding\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Multi-head self-attention with residual connection and layer norm\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stack of Transformer Encoder Layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        # Stack encoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, d_model)\n",
    "            mask: Optional mask for padding\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test Encoder\n",
    "print(\"Testing Transformer Encoder:\")\n",
    "num_layers = 2\n",
    "d_model = 512\n",
    "num_heads = 2\n",
    "d_ff = 2048\n",
    "\n",
    "encoder = TransformerEncoder(num_layers, d_model, num_heads, d_ff)\n",
    "\n",
    "dummy_input = torch.randn(2, 10, d_model)\n",
    "output = encoder(dummy_input)\n",
    "\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of encoder layers: {num_layers}\")\n",
    "print(\"‚úì Transformer Encoder implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa3117",
   "metadata": {},
   "source": [
    "## Section 12: Transformer Decoder Layer\n",
    "\n",
    "Build Decoder layer with:\n",
    "- Masked Multi-Head Self-Attention\n",
    "- Add & Norm\n",
    "- Multi-Head Cross-Attention (with Encoder output)\n",
    "- Add & Norm\n",
    "- Feed-Forward Network\n",
    "- Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Transformer Decoder Layer.\n",
    "    \n",
    "    Components:\n",
    "    1. Masked Multi-Head Self-Attention\n",
    "    2. Add & Norm\n",
    "    3. Multi-Head Cross-Attention (with encoder output)\n",
    "    4. Add & Norm\n",
    "    5. Feed-Forward Network\n",
    "    6. Add & Norm\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # Masked self-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Cross-attention with encoder output\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Decoder input (batch_size, tgt_seq_len, d_model)\n",
    "            encoder_output: Encoder output (batch_size, src_seq_len, d_model)\n",
    "            src_mask: Source mask for padding\n",
    "            tgt_mask: Target mask for padding and future positions\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor (batch_size, tgt_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Masked self-attention\n",
    "        self_attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(self_attn_output))\n",
    "        \n",
    "        # Cross-attention with encoder output\n",
    "        cross_attn_output, _ = self.cross_attn(x, encoder_output, encoder_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout2(cross_attn_output))\n",
    "        \n",
    "        # Feed-forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout3(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stack of Transformer Decoder Layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        # Stack decoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Decoder input (batch_size, tgt_seq_len, d_model)\n",
    "            encoder_output: Encoder output (batch_size, src_seq_len, d_model)\n",
    "            src_mask: Source mask\n",
    "            tgt_mask: Target mask\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor (batch_size, tgt_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test Decoder\n",
    "print(\"Testing Transformer Decoder:\")\n",
    "num_layers = 2\n",
    "d_model = 512\n",
    "num_heads = 2\n",
    "d_ff = 2048\n",
    "\n",
    "decoder = TransformerDecoder(num_layers, d_model, num_heads, d_ff)\n",
    "\n",
    "# Dummy inputs\n",
    "dummy_decoder_input = torch.randn(2, 8, d_model)\n",
    "dummy_encoder_output = torch.randn(2, 10, d_model)\n",
    "\n",
    "output = decoder(dummy_decoder_input, dummy_encoder_output)\n",
    "\n",
    "print(f\"Decoder input shape: {dummy_decoder_input.shape}\")\n",
    "print(f\"Encoder output shape: {dummy_encoder_output.shape}\")\n",
    "print(f\"Decoder output shape: {output.shape}\")\n",
    "print(f\"Number of decoder layers: {num_layers}\")\n",
    "print(\"‚úì Transformer Decoder implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc61a7ae",
   "metadata": {},
   "source": [
    "## Section 13: Complete Transformer Encoder-Decoder Model\n",
    "\n",
    "Assemble the full Transformer with:\n",
    "- Embedding layers\n",
    "- Positional encoding\n",
    "- Encoder stack\n",
    "- Decoder stack\n",
    "- Final linear projection to vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a75c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer Encoder-Decoder model built from scratch.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=512, num_heads=2, \n",
    "                 num_encoder_layers=2, num_decoder_layers=2,\n",
    "                 d_ff=2048, max_seq_len=5000, dropout=0.1, pad_idx=0):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.src_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.tgt_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
    "        \n",
    "        # Encoder and Decoder\n",
    "        self.encoder = TransformerEncoder(num_encoder_layers, d_model, num_heads, d_ff, dropout)\n",
    "        self.decoder = TransformerDecoder(num_decoder_layers, d_model, num_heads, d_ff, dropout)\n",
    "        \n",
    "        # Final projection to vocabulary\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights randomly.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                \n",
    "    def make_src_mask(self, src):\n",
    "        \"\"\"\n",
    "        Create mask for source padding.\n",
    "        \n",
    "        Args:\n",
    "            src: Source tensor (batch_size, src_seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            Mask tensor (batch_size, 1, 1, src_seq_len)\n",
    "        \"\"\"\n",
    "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "    \n",
    "    def make_tgt_mask(self, tgt):\n",
    "        \"\"\"\n",
    "        Create mask for target padding and future positions.\n",
    "        \n",
    "        Args:\n",
    "            tgt: Target tensor (batch_size, tgt_seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            Mask tensor (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
    "        \"\"\"\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        \n",
    "        # Padding mask\n",
    "        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        # Future mask (lower triangular matrix)\n",
    "        tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
    "        \n",
    "        # Combine masks\n",
    "        tgt_mask = tgt_pad_mask & tgt_sub_mask\n",
    "        \n",
    "        return tgt_mask\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Encode source sequence.\n",
    "        \n",
    "        Args:\n",
    "            src: Source indices (batch_size, src_seq_len)\n",
    "            src_mask: Source mask\n",
    "        \n",
    "        Returns:\n",
    "            Encoder output (batch_size, src_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Embed and add positional encoding\n",
    "        src_emb = self.src_embedding(src) * math.sqrt(self.d_model)\n",
    "        src_emb = self.pos_encoding(src_emb)\n",
    "        \n",
    "        # Encode\n",
    "        encoder_output = self.encoder(src_emb, src_mask)\n",
    "        \n",
    "        return encoder_output\n",
    "    \n",
    "    def decode(self, tgt, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Decode target sequence.\n",
    "        \n",
    "        Args:\n",
    "            tgt: Target indices (batch_size, tgt_seq_len)\n",
    "            encoder_output: Encoder output (batch_size, src_seq_len, d_model)\n",
    "            src_mask: Source mask\n",
    "            tgt_mask: Target mask\n",
    "        \n",
    "        Returns:\n",
    "            Decoder output (batch_size, tgt_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Embed and add positional encoding\n",
    "        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.pos_encoding(tgt_emb)\n",
    "        \n",
    "        # Decode\n",
    "        decoder_output = self.decoder(tgt_emb, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        return decoder_output\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            src: Source indices (batch_size, src_seq_len)\n",
    "            tgt: Target indices (batch_size, tgt_seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            Output logits (batch_size, tgt_seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        # Create masks\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "        \n",
    "        # Encode\n",
    "        encoder_output = self.encode(src, src_mask)\n",
    "        \n",
    "        # Decode\n",
    "        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        output = self.fc_out(decoder_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "print(\"‚úì Complete Transformer model implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9cc50d",
   "metadata": {},
   "source": [
    "## Section 14: PyTorch Dataset and DataLoader\n",
    "\n",
    "Create custom Dataset for handling input/output pairs with proper padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c454e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmpatheticDialogueDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for Empathetic Dialogue.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, vocab):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Create input and output pairs\n",
    "        input_text, output_text = create_input_output_pair(row)\n",
    "        \n",
    "        # Encode sequences\n",
    "        src_indices = encode_sequence(input_text, self.vocab, add_special_tokens=False)\n",
    "        tgt_indices = encode_sequence(output_text, self.vocab, add_special_tokens=True)\n",
    "        \n",
    "        return {\n",
    "            'src': torch.tensor(src_indices, dtype=torch.long),\n",
    "            'tgt': torch.tensor(tgt_indices, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch, pad_idx=0):\n",
    "    \"\"\"\n",
    "    Collate function for DataLoader to handle variable length sequences.\n",
    "    \"\"\"\n",
    "    src_batch = [item['src'] for item in batch]\n",
    "    tgt_batch = [item['tgt'] for item in batch]\n",
    "    \n",
    "    # Pad sequences\n",
    "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=pad_idx)\n",
    "    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=pad_idx)\n",
    "    \n",
    "    return {\n",
    "        'src': src_padded,\n",
    "        'tgt': tgt_padded\n",
    "    }\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = EmpatheticDialogueDataset(train_df, vocab)\n",
    "val_dataset = EmpatheticDialogueDataset(val_df, vocab)\n",
    "test_dataset = EmpatheticDialogueDataset(test_df, vocab)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Test dataset\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample from dataset:\")\n",
    "print(f\"Source shape: {sample['src'].shape}\")\n",
    "print(f\"Target shape: {sample['tgt'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e854eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders with platform-specific batch size for memory optimization\n",
    "if PLATFORM == \"local\":\n",
    "    BATCH_SIZE = 8   # Reduced batch size for local memory constraints (4GB GPU)\n",
    "    print(f\"üè† LOCAL platform detected: Using reduced batch size {BATCH_SIZE} for memory optimization\")\n",
    "else:\n",
    "    BATCH_SIZE = 64  # Full batch size for Kaggle/Cloud platforms\n",
    "    print(f\"‚òÅÔ∏è KAGGLE platform detected: Using full batch size {BATCH_SIZE}\")\n",
    "\n",
    "PAD_IDX = vocab.word2idx[vocab.PAD_TOKEN]\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: collate_fn(batch, PAD_IDX),\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda batch: collate_fn(batch, PAD_IDX),\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda batch: collate_fn(batch, PAD_IDX),\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"DataLoaders created successfully!\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")\n",
    "\n",
    "# Test a batch\n",
    "for batch in train_loader:\n",
    "    print(f\"\\nSample batch:\")\n",
    "    print(f\"Source batch shape: {batch['src'].shape}\")\n",
    "    print(f\"Target batch shape: {batch['tgt'].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3270f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEMORY OPTIMIZATION SUMMARY:\n",
    "# \n",
    "# ‚úÖ KAGGLE (Cloud): BATCH_SIZE = 64  ‚Üí Works with 16GB+ GPU memory\n",
    "# ‚úÖ LOCAL (4GB GPU): BATCH_SIZE = 8  ‚Üí Prevents kernel crashes due to memory constraints\n",
    "# \n",
    "# The model architecture (47M parameters) remains IDENTICAL on both platforms.\n",
    "# Only batch size is reduced for local execution to fit within 4GB GPU memory.\n",
    "# This ensures:\n",
    "# - No kernel crashes on local machine\n",
    "# - Same model performance and accuracy\n",
    "# - Compatible with your existing Kaggle-trained models\n",
    "# - Seamless switching between platforms\n",
    "#\n",
    "# Expected behavior:\n",
    "# - Local training: ~8x longer but stable (no crashes)  \n",
    "# - Local inference: Works perfectly with reduced batch size\n",
    "# - Kaggle: Full performance with batch size 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed39d6c",
   "metadata": {},
   "source": [
    "## Section 15: Model Initialization and Training Setup\n",
    "\n",
    "Initialize the Transformer model with specified hyperparameters and set up training components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb094623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters (as specified in requirements)\n",
    "MODEL_CONFIG = {\n",
    "    'vocab_size': len(vocab),\n",
    "    'd_model': 512,           # Embedding dimension: 256 or 512\n",
    "    'num_heads': 2,           # Number of attention heads: 2\n",
    "    'num_encoder_layers': 2,  # Encoder layers: 2 \n",
    "    'num_decoder_layers': 2,  # Decoder layers: 2\n",
    "    'd_ff': 2048,            # Feed-forward dimension (4 * d_model)\n",
    "    'max_seq_len': 1000,     # Maximum sequence length\n",
    "    'dropout': 0.1,          # Dropout: 0.1-0.3\n",
    "    'pad_idx': PAD_IDX\n",
    "}\n",
    "\n",
    "# Training hyperparameters (as specified in requirements)\n",
    "TRAIN_CONFIG = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'learning_rate': 1e-4,    # Learning rate: 1e-4 to 5e-4\n",
    "    'betas': (0.9, 0.98),     # Adam betas: (0.9, 0.98)\n",
    "    'num_epochs': 10,         # Number of training epochs\n",
    "    'warmup_steps': 4000,     # Learning rate warmup\n",
    "    'clip_grad_norm': 1.0,    # Gradient clipping\n",
    "    'save_every': 5,          # Save checkpoint every N epochs\n",
    "    'eval_every': 1,          # Evaluate every N epochs\n",
    "}\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "for key, value in MODEL_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")  \n",
    "for key, value in TRAIN_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "    \n",
    "print(f\"\\nDevice: {device}\")\n",
    "print(f\"Total parameters will be: ~{(MODEL_CONFIG['vocab_size'] * MODEL_CONFIG['d_model'] * 2 + MODEL_CONFIG['d_model'] ** 2 * 12) // 1000}K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7520747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Transformer model\n",
    "print(\"Initializing Transformer model...\")\n",
    "model = Transformer(**MODEL_CONFIG).to(device)\n",
    "\n",
    "# Count total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / (1024**2):.1f} MB\")\n",
    "\n",
    "# Initialize optimizer (Adam with specified betas)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=TRAIN_CONFIG['learning_rate'], \n",
    "    betas=TRAIN_CONFIG['betas']\n",
    ")\n",
    "\n",
    "# Loss function (Cross Entropy with padding ignored)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# Learning rate scheduler with warmup (FIXED: Added state_dict methods)\n",
    "class WarmupScheduler:\n",
    "    def __init__(self, optimizer, d_model, warmup_steps):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def step(self):\n",
    "        self.step_count += 1\n",
    "        lr = self.d_model ** (-0.5) * min(\n",
    "            self.step_count ** (-0.5),\n",
    "            self.step_count * self.warmup_steps ** (-1.5)\n",
    "        )\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    def state_dict(self):\n",
    "        \"\"\"Return the state of the scheduler.\"\"\"\n",
    "        return {\n",
    "            'step_count': self.step_count,\n",
    "            'd_model': self.d_model,\n",
    "            'warmup_steps': self.warmup_steps\n",
    "        }\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"Load the state of the scheduler.\"\"\"\n",
    "        self.step_count = state_dict.get('step_count', 0)\n",
    "        self.d_model = state_dict.get('d_model', self.d_model)\n",
    "        self.warmup_steps = state_dict.get('warmup_steps', self.warmup_steps)\n",
    "\n",
    "scheduler = WarmupScheduler(optimizer, MODEL_CONFIG['d_model'], TRAIN_CONFIG['warmup_steps'])\n",
    "\n",
    "print(f\"\\n‚úì Model, optimizer, loss function, and scheduler initialized!\")\n",
    "print(f\"‚úì All weights randomly initialized (no pretrained weights used)\")\n",
    "print(f\"üîß FIXED: WarmupScheduler now has state_dict() and load_state_dict() methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cae5a62",
   "metadata": {},
   "source": [
    "## Section 16: Evaluation Metrics Implementation\n",
    "\n",
    "Implement BLEU, ROUGE-L, chrF, and Perplexity metrics from scratch for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375179eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def calculate_bleu(candidate, reference, n=4):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score (BiLingual Evaluation Understudy).\n",
    "    \n",
    "    Args:\n",
    "        candidate: Generated text (list of tokens)\n",
    "        reference: Ground truth text (list of tokens) \n",
    "        n: Maximum n-gram order (default: 4)\n",
    "    \n",
    "    Returns:\n",
    "        BLEU score (0-1)\n",
    "    \"\"\"\n",
    "    if len(candidate) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate n-gram precisions\n",
    "    precisions = []\n",
    "    for i in range(1, n + 1):\n",
    "        # Get n-grams\n",
    "        cand_ngrams = Counter([tuple(candidate[j:j+i]) for j in range(len(candidate)-i+1)])\n",
    "        ref_ngrams = Counter([tuple(reference[j:j+i]) for j in range(len(reference)-i+1)])\n",
    "        \n",
    "        # Calculate precision\n",
    "        match_count = sum(min(cand_ngrams[ngram], ref_ngrams[ngram]) for ngram in cand_ngrams)\n",
    "        total_count = sum(cand_ngrams.values())\n",
    "        \n",
    "        if total_count == 0:\n",
    "            precision = 0.0\n",
    "        else:\n",
    "            precision = match_count / total_count\n",
    "        precisions.append(precision)\n",
    "    \n",
    "    # Geometric mean of precisions\n",
    "    if min(precisions) > 0:\n",
    "        geo_mean = (precisions[0] * precisions[1] * precisions[2] * precisions[3]) ** 0.25\n",
    "    else:\n",
    "        geo_mean = 0.0\n",
    "    \n",
    "    # Brevity penalty\n",
    "    c_len = len(candidate)\n",
    "    r_len = len(reference) \n",
    "    \n",
    "    if c_len > r_len:\n",
    "        bp = 1.0\n",
    "    else:\n",
    "        bp = math.exp(1 - r_len / c_len) if c_len > 0 else 0.0\n",
    "    \n",
    "    bleu = bp * geo_mean\n",
    "    return bleu\n",
    "\n",
    "\n",
    "def lcs_length(X, Y):\n",
    "    \"\"\"Calculate Longest Common Subsequence length.\"\"\"\n",
    "    m, n = len(X), len(Y)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if X[i-1] == Y[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "    \n",
    "    return dp[m][n]\n",
    "\n",
    "\n",
    "def calculate_rouge_l(candidate, reference):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE-L score (Recall-Oriented Understudy for Gisting Evaluation - Longest).\n",
    "    \n",
    "    Args:\n",
    "        candidate: Generated text (list of tokens)\n",
    "        reference: Ground truth text (list of tokens)\n",
    "    \n",
    "    Returns:\n",
    "        ROUGE-L F1 score (0-1)\n",
    "    \"\"\"\n",
    "    if len(candidate) == 0 or len(reference) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    lcs_len = lcs_length(candidate, reference)\n",
    "    \n",
    "    recall = lcs_len / len(reference) if len(reference) > 0 else 0.0\n",
    "    precision = lcs_len / len(candidate) if len(candidate) > 0 else 0.0\n",
    "    \n",
    "    if recall + precision == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * recall * precision / (recall + precision)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "\n",
    "def calculate_chrf(candidate, reference, n=6, beta=2):\n",
    "    \"\"\"\n",
    "    Calculate chrF score (Character n-gram F-score).\n",
    "    \n",
    "    Args:\n",
    "        candidate: Generated text (string)\n",
    "        reference: Ground truth text (string)\n",
    "        n: Maximum character n-gram order (default: 6)\n",
    "        beta: Beta parameter for F-score (default: 2)\n",
    "    \n",
    "    Returns:\n",
    "        chrF score (0-100)\n",
    "    \"\"\"\n",
    "    if len(candidate) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Convert to character level\n",
    "    candidate_chars = list(candidate)\n",
    "    reference_chars = list(reference)\n",
    "    \n",
    "    total_recall = 0\n",
    "    total_precision = 0\n",
    "    \n",
    "    for i in range(1, n + 1):\n",
    "        # Get character n-grams\n",
    "        cand_ngrams = Counter([tuple(candidate_chars[j:j+i]) for j in range(len(candidate_chars)-i+1)])\n",
    "        ref_ngrams = Counter([tuple(reference_chars[j:j+i]) for j in range(len(reference_chars)-i+1)])\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        match_count = sum(min(cand_ngrams[ngram], ref_ngrams[ngram]) for ngram in cand_ngrams)\n",
    "        \n",
    "        precision = match_count / sum(cand_ngrams.values()) if sum(cand_ngrams.values()) > 0 else 0\n",
    "        recall = match_count / sum(ref_ngrams.values()) if sum(ref_ngrams.values()) > 0 else 0\n",
    "        \n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "    \n",
    "    # Average precision and recall\n",
    "    avg_precision = total_precision / n\n",
    "    avg_recall = total_recall / n\n",
    "    \n",
    "    # F-score with beta\n",
    "    if avg_precision + avg_recall == 0:\n",
    "        f_score = 0.0\n",
    "    else:\n",
    "        f_score = (1 + beta**2) * avg_precision * avg_recall / (beta**2 * avg_precision + avg_recall)\n",
    "    \n",
    "    return f_score * 100  # Return as percentage\n",
    "\n",
    "\n",
    "def calculate_perplexity(loss):\n",
    "    \"\"\"\n",
    "    Calculate perplexity from cross-entropy loss.\n",
    "    \n",
    "    Args:\n",
    "        loss: Cross-entropy loss value\n",
    "    \n",
    "    Returns:\n",
    "        Perplexity score\n",
    "    \"\"\"\n",
    "    return math.exp(loss)\n",
    "\n",
    "\n",
    "print(\"‚úì Evaluation metrics implemented:\")\n",
    "print(\"  - BLEU score (n-gram precision with brevity penalty)\")\n",
    "print(\"  - ROUGE-L score (longest common subsequence F1)\")\n",
    "print(\"  - chrF score (character n-gram F-score)\")\n",
    "print(\"  - Perplexity (exponential of cross-entropy loss)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e02d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Resumption Functions\n",
    "def find_latest_checkpoint(checkpoint_dir='saved_models'):\n",
    "    \"\"\"\n",
    "    Find the latest checkpoint file to resume training from.\n",
    "    \n",
    "    Returns:\n",
    "        str or None: Path to latest checkpoint file, None if no checkpoints found\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import glob\n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return None\n",
    "    \n",
    "    # Look for checkpoint files\n",
    "    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'checkpoint_epoch_*.pkl'))\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    \n",
    "    # Extract epoch numbers and find the latest\n",
    "    epoch_numbers = []\n",
    "    for file_path in checkpoint_files:\n",
    "        try:\n",
    "            # Extract epoch number from filename like 'checkpoint_epoch_5.pkl'\n",
    "            filename = os.path.basename(file_path)\n",
    "            epoch_str = filename.replace('checkpoint_epoch_', '').replace('.pkl', '')\n",
    "            epoch_numbers.append((int(epoch_str), file_path))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    if not epoch_numbers:\n",
    "        return None\n",
    "    \n",
    "    # Return path to checkpoint with highest epoch number\n",
    "    latest_epoch, latest_path = max(epoch_numbers, key=lambda x: x[0])\n",
    "    print(f\"üìÅ Found latest checkpoint: Epoch {latest_epoch} at {latest_path}\")\n",
    "    return latest_path\n",
    "\n",
    "\n",
    "def load_checkpoint_for_resume(checkpoint_path, model, optimizer, scheduler, device):\n",
    "    \"\"\"\n",
    "    Load checkpoint and restore training state.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to checkpoint file\n",
    "        model: Model to load state into\n",
    "        optimizer: Optimizer to load state into\n",
    "        scheduler: Scheduler to load state into\n",
    "        device: Device to load tensors to\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (start_epoch, training_history, best_bleu, best_epoch)\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Loading checkpoint from: {checkpoint_path}\")\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        # Load model state\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"   ‚úì Model state loaded\")\n",
    "        \n",
    "        # Load optimizer state\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(f\"   ‚úì Optimizer state loaded\")\n",
    "        \n",
    "        # Load scheduler state\n",
    "        if 'scheduler_state_dict' in checkpoint and scheduler is not None:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            print(f\"   ‚úì Scheduler state loaded\")\n",
    "        \n",
    "        # Load training progress\n",
    "        start_epoch = checkpoint['epoch'] + 1  # Start from next epoch\n",
    "        training_history = checkpoint.get('training_history', {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [], \n",
    "            'val_perplexity': [],\n",
    "            'val_bleu': [],\n",
    "            'val_rouge_l': [],\n",
    "            'val_chrf': []\n",
    "        })\n",
    "        \n",
    "        best_bleu = checkpoint.get('best_bleu', 0.0)\n",
    "        best_epoch = checkpoint.get('best_epoch', 0)\n",
    "        \n",
    "        print(f\"   ‚úì Resuming from epoch {start_epoch}\")\n",
    "        print(f\"   ‚úì Previous best BLEU: {best_bleu:.4f} (Epoch {best_epoch})\")\n",
    "        print(f\"   ‚úì Training history loaded: {len(training_history['train_loss'])} previous epochs\")\n",
    "        \n",
    "        return start_epoch, training_history, best_bleu, best_epoch\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error loading checkpoint: {str(e)}\")\n",
    "        print(f\"   üîÑ Starting fresh training instead...\")\n",
    "        return 1, {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [], \n",
    "            'val_perplexity': [],\n",
    "            'val_bleu': [],\n",
    "            'val_rouge_l': [],\n",
    "            'val_chrf': []\n",
    "        }, 0.0, 0\n",
    "\n",
    "\n",
    "def save_training_checkpoint(epoch, model, optimizer, scheduler, training_history, \n",
    "                           best_bleu, best_epoch, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Save comprehensive training checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        epoch: Current epoch number\n",
    "        model: Current model state\n",
    "        optimizer: Current optimizer state  \n",
    "        scheduler: Current scheduler state\n",
    "        training_history: Training metrics history\n",
    "        best_bleu: Best BLEU score so far\n",
    "        best_epoch: Epoch with best BLEU score\n",
    "        checkpoint_path: Path to save checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'training_history': training_history,\n",
    "        'best_bleu': best_bleu,\n",
    "        'best_epoch': best_epoch,\n",
    "        'model_config': MODEL_CONFIG,\n",
    "        'train_config': TRAIN_CONFIG,\n",
    "        'vocab_size': len(vocab),\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"   ‚úÖ Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Training resumption functions implemented!\")\n",
    "print(\"   üîÑ find_latest_checkpoint(): Finds most recent checkpoint\")\n",
    "print(\"   üìÇ load_checkpoint_for_resume(): Loads complete training state\")  \n",
    "print(\"   üíæ save_training_checkpoint(): Saves comprehensive checkpoints\")\n",
    "print(\"   üõ°Ô∏è Handles interruptions and automatic resumption\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ff7fc",
   "metadata": {},
   "source": [
    "## Section 17A: Training Resumption Guide\n",
    "\n",
    "### üîÑ **Automatic Training Resumption**\n",
    "\n",
    "The training system now supports automatic resumption from interruptions:\n",
    "\n",
    "#### **Features:**\n",
    "- ‚úÖ **Automatic checkpoint detection** - Finds latest checkpoint on restart\n",
    "- ‚úÖ **Complete state restoration** - Model, optimizer, scheduler, training history\n",
    "- ‚úÖ **Interruption handling** - Saves emergency checkpoints on Ctrl+C\n",
    "- ‚úÖ **Error recovery** - Saves checkpoints even on unexpected errors\n",
    "- ‚úÖ **Progress preservation** - Maintains best BLEU scores and training metrics\n",
    "\n",
    "#### **How It Works:**\n",
    "1. **On Training Start**: Automatically checks for existing checkpoints\n",
    "2. **If Found**: Loads complete training state and resumes from next epoch\n",
    "3. **If Not Found**: Starts fresh training from epoch 1\n",
    "4. **During Training**: Saves checkpoints every N epochs + after validation\n",
    "5. **On Interruption**: Saves emergency checkpoint for safe resumption\n",
    "\n",
    "#### **Checkpoint Files:**\n",
    "- `checkpoint_epoch_N.pkl` - Regular training checkpoints\n",
    "- `emergency_checkpoint_epoch_N.pkl` - Saved on Ctrl+C interruption  \n",
    "- `error_checkpoint_epoch_N.pkl` - Saved on unexpected errors\n",
    "- `best_model.pkl` - Best performing model (highest BLEU score)\n",
    "\n",
    "#### **Manual Resumption:**\n",
    "If needed, you can manually specify which checkpoint to resume from by modifying the checkpoint loading logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e0d690",
   "metadata": {},
   "source": [
    "## Section 17: Training Loop with Teacher Forcing\n",
    "\n",
    "Implement the complete training loop that will take several hours to complete. This includes model checkpointing and metric tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c13269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, scheduler, device, epoch):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch with teacher forcing.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch}', leave=False)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        src = batch['src'].to(device)  # (batch_size, src_seq_len)\n",
    "        tgt = batch['tgt'].to(device)  # (batch_size, tgt_seq_len)\n",
    "        \n",
    "        # Teacher forcing: use target input (all tokens except last)\n",
    "        tgt_input = tgt[:, :-1]  # (batch_size, tgt_seq_len-1)\n",
    "        tgt_output = tgt[:, 1:]  # (batch_size, tgt_seq_len-1)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Model output: (batch_size, tgt_seq_len-1, vocab_size)\n",
    "        output = model(src, tgt_input)\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        output = output.reshape(-1, output.size(-1))  # (batch_size * seq_len, vocab_size)\n",
    "        tgt_output = tgt_output.reshape(-1)           # (batch_size * seq_len)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, tgt_output)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), TRAIN_CONFIG['clip_grad_norm'])\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'avg_loss': f'{total_loss/num_batches:.4f}',\n",
    "            'lr': f'{optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "        })\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def evaluate_model(model, val_loader, criterion, device, vocab):\n",
    "    \"\"\"\n",
    "    Evaluate the model on validation set and calculate metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(val_loader, desc='Evaluating', leave=False)):\n",
    "            src = batch['src'].to(device)\n",
    "            tgt = batch['tgt'].to(device)\n",
    "            \n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src, tgt_input)\n",
    "            \n",
    "            # Calculate loss\n",
    "            output_flat = output.reshape(-1, output.size(-1))\n",
    "            tgt_output_flat = tgt_output.reshape(-1)\n",
    "            loss = criterion(output_flat, tgt_output_flat)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Generate predictions for metrics\n",
    "            predictions = output.argmax(dim=-1)  # (batch_size, seq_len)\n",
    "            \n",
    "            # Convert to text for metric calculation (first 5 samples)\n",
    "            for i in range(min(5, predictions.size(0))):\n",
    "                pred_tokens = [vocab.get_word(idx.item()) for idx in predictions[i]]\n",
    "                ref_tokens = [vocab.get_word(idx.item()) for idx in tgt_output[i]]\n",
    "                \n",
    "                # Remove padding tokens\n",
    "                pred_tokens = [tok for tok in pred_tokens if tok != vocab.PAD_TOKEN]\n",
    "                ref_tokens = [tok for tok in ref_tokens if tok != vocab.PAD_TOKEN]\n",
    "                \n",
    "                all_predictions.append(pred_tokens)\n",
    "                all_references.append(ref_tokens)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = calculate_perplexity(avg_loss)\n",
    "    \n",
    "    # Calculate BLEU, ROUGE-L, chrF on sample predictions\n",
    "    bleu_scores = [calculate_bleu(pred, ref) for pred, ref in zip(all_predictions, all_references)]\n",
    "    rouge_scores = [calculate_rouge_l(pred, ref) for pred, ref in zip(all_predictions, all_references)]\n",
    "    chrf_scores = [calculate_chrf(' '.join(pred), ' '.join(ref)) for pred, ref in zip(all_predictions, all_references)]\n",
    "    \n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
    "    avg_rouge = sum(rouge_scores) / len(rouge_scores) if rouge_scores else 0\n",
    "    avg_chrf = sum(chrf_scores) / len(chrf_scores) if chrf_scores else 0\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'perplexity': perplexity,\n",
    "        'bleu': avg_bleu,\n",
    "        'rouge_l': avg_rouge,\n",
    "        'chrf': avg_chrf\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úì Training and evaluation functions defined!\")\n",
    "print(\"  - train_one_epoch(): Implements teacher forcing training\")\n",
    "print(\"  - evaluate_model(): Calculates all metrics (BLEU, ROUGE-L, chrF, Perplexity)\")\n",
    "print(\"  - Includes gradient clipping and learning rate scheduling\")\n",
    "print(\"  - Ready for long training process with checkpointing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404bd9ff",
   "metadata": {},
   "source": [
    "## ‚úÖ **Platform-Aware Full Dataset Training**\n",
    "\n",
    "### üéØ **Key Changes Made:**\n",
    "\n",
    "1. **‚úÖ REMOVED Memory Optimizations** - Training now uses FULL model capacity:\n",
    "   - Original model: **512 d_model**, **2048 d_ff**, **47M parameters**\n",
    "   - Original batch size: **64** \n",
    "   - Original sequence length: **1000**\n",
    "\n",
    "2. **‚úÖ ADDED Platform Detection** - Works on both Local & Kaggle:\n",
    "   - üè† **Local**: Searches current directory for `emotion-emotion_69k.csv`\n",
    "   - ‚òÅÔ∏è **Kaggle**: Auto-detects dataset in `/kaggle/input/` directories\n",
    "   - üìÅ **Outputs**: Platform-aware saving (local dir vs `/kaggle/working/`)\n",
    "\n",
    "3. **‚úÖ FULL Dataset Training** - No compromises:\n",
    "   - Complete **51,672 training samples** per epoch\n",
    "   - Full **6,459 validation samples** \n",
    "   - All **808 batches** processed (no early breaks)\n",
    "   - Complete model architecture with resumption support\n",
    "\n",
    "### üöÄ **Training Features:**\n",
    "- ‚úÖ **Automatic checkpoint resumption** from interruptions\n",
    "- ‚úÖ **Platform-aware file paths** (local vs Kaggle)\n",
    "- ‚úÖ **Complete dataset processing** (no memory reduction)\n",
    "- ‚úÖ **Full model capacity** (47M parameters)\n",
    "- ‚úÖ **Robust error handling** with emergency saves\n",
    "\n",
    "**Ready for full-scale training on complete dataset!** üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2447eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL DATASET TRAINING - NO MEMORY OPTIMIZATION\n",
    "# Check for existing checkpoints and resume if possible\n",
    "print(\"üîç Checking for existing checkpoints...\")\n",
    "checkpoint_dir = get_platform_path('saved_models')\n",
    "latest_checkpoint = find_latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "if latest_checkpoint:\n",
    "    print(\"‚úÖ Found existing checkpoint - resuming training...\")\n",
    "    start_epoch, training_history, best_bleu, best_epoch = load_checkpoint_for_resume(\n",
    "        latest_checkpoint, model, optimizer, scheduler, device\n",
    "    )\n",
    "    total_epochs = TRAIN_CONFIG['num_epochs']\n",
    "    remaining_epochs = total_epochs - (start_epoch - 1)\n",
    "    print(f\"üìä Resuming: {remaining_epochs} epochs remaining out of {total_epochs} total\")\n",
    "else:\n",
    "    print(\"üÜï No existing checkpoints found - starting fresh training...\")\n",
    "    # Training tracking variables\n",
    "    training_history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [], \n",
    "        'val_perplexity': [],\n",
    "        'val_bleu': [],\n",
    "        'val_rouge_l': [],\n",
    "        'val_chrf': []\n",
    "    }\n",
    "    best_bleu = 0.0\n",
    "    best_epoch = 0\n",
    "    start_epoch = 1\n",
    "\n",
    "print(\"Starting/Resuming Training Process...\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  - Total Epochs: {TRAIN_CONFIG['num_epochs']}\")\n",
    "print(f\"  - Starting from Epoch: {start_epoch}\")\n",
    "print(f\"  - Batch Size: {TRAIN_CONFIG['batch_size']}\")\n",
    "print(f\"  - Learning Rate: {TRAIN_CONFIG['learning_rate']}\")\n",
    "print(f\"  - Model Parameters: {trainable_params:,}\")\n",
    "print(f\"  - Training Samples: {len(train_dataset):,}\")\n",
    "print(f\"  - Validation Samples: {len(val_dataset):,}\")\n",
    "print(f\"  - Current Best BLEU: {best_bleu:.4f}\")\n",
    "print(f\"  - üíæ AUTOMATIC SAVE: After every epoch completion!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Main training loop with interruption handling\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    for epoch in range(start_epoch, TRAIN_CONFIG['num_epochs'] + 1):\n",
    "        print(f\"\\nüìç Epoch {epoch}/{TRAIN_CONFIG['num_epochs']}\")\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scheduler, device, epoch\n",
    "        )\n",
    "        \n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        \n",
    "        print(f\"  Training Loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        if epoch % TRAIN_CONFIG['eval_every'] == 0:\n",
    "            print(\"  Evaluating on validation set...\")\n",
    "            val_metrics = evaluate_model(model, val_loader, criterion, device, vocab)\n",
    "            \n",
    "            # Store metrics\n",
    "            training_history['val_loss'].append(val_metrics['loss'])\n",
    "            training_history['val_perplexity'].append(val_metrics['perplexity'])\n",
    "            training_history['val_bleu'].append(val_metrics['bleu'])\n",
    "            training_history['val_rouge_l'].append(val_metrics['rouge_l'])\n",
    "            training_history['val_chrf'].append(val_metrics['chrf'])\n",
    "            \n",
    "            print(f\"  Validation Loss: {val_metrics['loss']:.4f}\")\n",
    "            print(f\"  Perplexity: {val_metrics['perplexity']:.2f}\")\n",
    "            print(f\"  BLEU: {val_metrics['bleu']:.4f}\")\n",
    "            print(f\"  ROUGE-L: {val_metrics['rouge_l']:.4f}\")\n",
    "            print(f\"  chrF: {val_metrics['chrf']:.2f}\")\n",
    "            \n",
    "            # Save best model based on BLEU score\n",
    "            if val_metrics['bleu'] > best_bleu:\n",
    "                best_bleu = val_metrics['bleu']\n",
    "                best_epoch = epoch\n",
    "            \n",
    "                # Save best model checkpoint\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'best_bleu': best_bleu,\n",
    "                    'best_epoch': best_epoch,\n",
    "                    'val_metrics': val_metrics,\n",
    "                    'model_config': MODEL_CONFIG,\n",
    "                    'train_config': TRAIN_CONFIG,\n",
    "                    'training_history': training_history,\n",
    "                    'vocab': vocab\n",
    "                }\n",
    "                \n",
    "                best_model_path = get_platform_path('saved_models/best_model.pkl')\n",
    "                os.makedirs(os.path.dirname(best_model_path), exist_ok=True)\n",
    "                torch.save(checkpoint, best_model_path)\n",
    "                print(f\"  ‚úì New best model saved! (BLEU: {best_bleu:.4f})\")\n",
    "        \n",
    "        # üíæ SAVE CHECKPOINT AFTER EVERY EPOCH COMPLETION (as requested by user)\n",
    "        print(f\"  üíæ Saving checkpoint for epoch {epoch}...\")\n",
    "        checkpoint_path = get_platform_path(f'saved_models/checkpoint_epoch_{epoch}.pkl')\n",
    "        save_training_checkpoint(\n",
    "            epoch, model, optimizer, scheduler, training_history,\n",
    "            best_bleu, best_epoch, checkpoint_path\n",
    "        )\n",
    "        print(f\"  ‚úÖ Epoch {epoch} checkpoint saved successfully!\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\nüõë Training interrupted by user at epoch {epoch}\")\n",
    "    print(f\"üíæ Saving emergency checkpoint...\")\n",
    "    emergency_path = get_platform_path(f'saved_models/emergency_checkpoint_epoch_{epoch}.pkl')\n",
    "    save_training_checkpoint(\n",
    "        epoch, model, optimizer, scheduler, training_history,\n",
    "        best_bleu, best_epoch, emergency_path\n",
    "    )\n",
    "    print(f\"‚úÖ Emergency checkpoint saved! Can resume from epoch {epoch + 1}\")\n",
    "    raise\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training failed with error: {str(e)}\")\n",
    "    print(f\"üíæ Saving error checkpoint...\")\n",
    "    error_path = get_platform_path(f'saved_models/error_checkpoint_epoch_{epoch}.pkl')\n",
    "    save_training_checkpoint(\n",
    "        epoch, model, optimizer, scheduler, training_history,\n",
    "        best_bleu, best_epoch, error_path\n",
    "    )\n",
    "    print(f\"‚úÖ Error checkpoint saved! Can investigate and resume from epoch {epoch + 1}\")\n",
    "    raise\n",
    "\n",
    "# Training completed successfully\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nüéâ Training Completed Successfully!\")\n",
    "print(f\"  Total Time: {total_time/3600:.2f} hours\")\n",
    "print(f\"  Best BLEU Score: {best_bleu:.4f} (Epoch {best_epoch})\")\n",
    "print(f\"  Total Epochs Completed: {TRAIN_CONFIG['num_epochs']}\")\n",
    "\n",
    "# Save final training history\n",
    "os.makedirs('saved_data', exist_ok=True)\n",
    "history_path = get_platform_path('saved_data/training_history.pkl')\n",
    "with open(history_path, 'wb') as f:\n",
    "    pickle.dump(training_history, f)\n",
    "print(f\"  ‚úì Final training history saved to 'saved_data/training_history.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfb4859",
   "metadata": {},
   "source": [
    "## Section 18: Inference and Response Generation\n",
    "\n",
    "Load the best trained model and implement response generation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b054d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model(model_path='saved_models/best_model.pkl', device='cpu'):\n",
    "    \"\"\"\n",
    "    Load the best trained model from checkpoint.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # FIXED: Use weights_only=False for custom classes (PyTorch 2.6+ compatibility)\n",
    "        checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    except Exception as e:\n",
    "        if \"weights_only\" in str(e):\n",
    "            # Alternative fix: Add safe globals for custom classes\n",
    "            torch.serialization.add_safe_globals([Vocabulary, Transformer])\n",
    "            checkpoint = torch.load(model_path, map_location=device)\n",
    "        else:\n",
    "            raise e\n",
    "    \n",
    "    # Recreate model with saved config\n",
    "    loaded_vocab = checkpoint['vocab']\n",
    "    model_config = checkpoint['model_config']\n",
    "    \n",
    "    model = Transformer(**model_config).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"‚úì Best model loaded from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"  BLEU Score: {checkpoint['best_bleu']:.4f}\")\n",
    "    \n",
    "    return model, loaded_vocab, checkpoint['val_metrics']\n",
    "\n",
    "\n",
    "def generate_response(model, emotion, situation, customer_utterance, vocab, device, max_length=50):\n",
    "    \"\"\"\n",
    "    Generate empathetic agent response using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Transformer model\n",
    "        emotion: Emotion category (string)\n",
    "        situation: Situation description (string) \n",
    "        customer_utterance: Customer's message (string)\n",
    "        vocab: Vocabulary object\n",
    "        device: Device (cpu/cuda)\n",
    "        max_length: Maximum response length\n",
    "        \n",
    "    Returns:\n",
    "        Generated agent response (string)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Normalize inputs\n",
    "        emotion = emotion.lower()\n",
    "        situation = normalize_text(situation)\n",
    "        customer_utterance = normalize_text(customer_utterance)\n",
    "        \n",
    "        # Create input format\n",
    "        input_text = f\"emotion : {emotion} | situation : {situation} | customer : {customer_utterance} agent :\"\n",
    "        \n",
    "        # Encode input\n",
    "        src_tokens = encode_sequence(input_text, vocab, add_special_tokens=False)\n",
    "        src = torch.tensor([src_tokens], dtype=torch.long).to(device)\n",
    "        \n",
    "        # Start with BOS token\n",
    "        tgt = torch.tensor([[vocab.word2idx[vocab.BOS_TOKEN]]], dtype=torch.long).to(device)\n",
    "        \n",
    "        # Generate tokens one by one\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass\n",
    "            output = model(src, tgt)\n",
    "            \n",
    "            # Get next token probabilities\n",
    "            next_token_logits = output[0, -1, :]  # Last token predictions\n",
    "            next_token = next_token_logits.argmax().item()\n",
    "            \n",
    "            # Stop if EOS token generated\n",
    "            if next_token == vocab.word2idx[vocab.EOS_TOKEN]:\n",
    "                break\n",
    "                \n",
    "            # Append to target\n",
    "            tgt = torch.cat([tgt, torch.tensor([[next_token]], device=device)], dim=1)\n",
    "        \n",
    "        # Convert to text\n",
    "        generated_tokens = tgt[0][1:].tolist()  # Remove BOS token\n",
    "        response_tokens = [vocab.get_word(idx) for idx in generated_tokens]\n",
    "        \n",
    "        # Remove EOS and PAD tokens\n",
    "        response_tokens = [token for token in response_tokens \n",
    "                          if token not in [vocab.EOS_TOKEN, vocab.PAD_TOKEN, vocab.UNK_TOKEN]]\n",
    "        \n",
    "        return ' '.join(response_tokens)\n",
    "\n",
    "\n",
    "print(\"‚úì Inference functions implemented!\")\n",
    "print(\"  - load_best_model(): Loads trained model from .pkl file (FIXED: PyTorch 2.6+ compatibility)\")\n",
    "print(\"  - generate_response(): Generates empathetic responses\")\n",
    "print(\"  - Uses greedy decoding with early stopping on EOS token\")\n",
    "print(\"  - Alternative fix: Safe globals for custom classes added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937d9a18",
   "metadata": {},
   "source": [
    "## Section 19: Qualitative Examples and Analysis\n",
    "\n",
    "Generate sample conversations and compare with ground truth responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load trained model if it exists, otherwise use current model\n",
    "try:\n",
    "    # FIXED: Use platform-aware path\n",
    "    model_path = get_platform_path('saved_models/best_model.pkl')\n",
    "    trained_model, model_vocab, val_metrics = load_best_model(model_path, device)\n",
    "    print(\"Using trained model for inference...\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No trained model found. Using current initialized model for demo...\")\n",
    "    trained_model = model\n",
    "    model_vocab = vocab\n",
    "    val_metrics = None\n",
    "\n",
    "# Sample test cases for qualitative evaluation (REAL DATASET EXAMPLES)\n",
    "test_cases = [\n",
    "    {\n",
    "        'emotion': 'sentimental',\n",
    "        'situation': 'I remember going to the fireworks with my best friend. There was a lot of people, but it only felt like us in the world.',\n",
    "        'customer': 'I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.',\n",
    "        'ground_truth': 'Was this a friend you were in love with, or just a best friend?'\n",
    "    },\n",
    "    {\n",
    "        'emotion': 'afraid', \n",
    "        'situation': 'i used to scare for darkness',\n",
    "        'customer': 'it feels like hitting to blank wall when i see the darkness',\n",
    "        'ground_truth': 'Oh ya? I don\\'t really see how'\n",
    "    },\n",
    "    {\n",
    "        'emotion': 'proud',\n",
    "        'situation': 'I showed a guy how to run a good bead in welding class and he caught on quick.',\n",
    "        'customer': 'Hi how are you doing today',\n",
    "        'ground_truth': 'doing good.. how about you'\n",
    "    },\n",
    "    {\n",
    "        'emotion': 'joyful',\n",
    "        'situation': 'I am very happy to have been first over 300 students during this years at my enginering school',\n",
    "        'customer': 'Hi, this year, I was the first over 300 students at my enginering school',\n",
    "        'ground_truth': 'Sounds great! So what\\'s your major?'\n",
    "    },\n",
    "    {\n",
    "        'emotion': 'lonely',\n",
    "        'situation': 'A few years ago, my marriage broke up, and I found myself living alone for the first time in my life. Though I eventually grew accustomed to the solitude, it took a while to get used to it.',\n",
    "        'customer': 'I found myself divorced a few years ago, and for the first time in my life, I was living alone.',\n",
    "        'ground_truth': 'I felt sad and depressed due to my insecurities and felt rejected from society'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Generating Qualitative Examples:\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    print(f\"\\nüìù Example {i}: {test_case['emotion'].title()} Conversation\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    print(f\"Emotion: {test_case['emotion']}\")\n",
    "    print(f\"Situation: {test_case['situation']}\")\n",
    "    print(f\"Customer: {test_case['customer']}\")\n",
    "    print()\n",
    "    \n",
    "    # Generate model response\n",
    "    generated_response = generate_response(\n",
    "        trained_model, \n",
    "        test_case['emotion'],\n",
    "        test_case['situation'], \n",
    "        test_case['customer'],\n",
    "        model_vocab,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    print(f\"Ground Truth: {test_case['ground_truth']}\")\n",
    "    print(f\"Generated   : {generated_response}\")\n",
    "    \n",
    "    # Calculate metrics for this example\n",
    "    gt_tokens = simple_tokenize(normalize_text(test_case['ground_truth']))\n",
    "    gen_tokens = simple_tokenize(generated_response)\n",
    "    \n",
    "    bleu = calculate_bleu(gen_tokens, gt_tokens)\n",
    "    rouge_l = calculate_rouge_l(gen_tokens, gt_tokens) \n",
    "    chrf = calculate_chrf(generated_response, test_case['ground_truth'])\n",
    "    \n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"  BLEU: {bleu:.3f} | ROUGE-L: {rouge_l:.3f} | chrF: {chrf:.1f}\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "print(f\"\\nüéØ Qualitative Analysis Complete!\")\n",
    "if val_metrics:\n",
    "    print(f\"Overall Model Performance (Validation Set):\")\n",
    "    print(f\"  BLEU: {val_metrics['bleu']:.4f}\")\n",
    "    print(f\"  ROUGE-L: {val_metrics['rouge_l']:.4f}\") \n",
    "    print(f\"  chrF: {val_metrics['chrf']:.2f}\")\n",
    "    print(f\"  Perplexity: {val_metrics['perplexity']:.2f}\")\n",
    "else:\n",
    "    print(f\"Note: Model not yet trained. Run training cells first for accurate metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45115eab",
   "metadata": {},
   "source": [
    "## Section 20: Training Progress Visualization\n",
    "\n",
    "Visualize training metrics and model performance over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d788e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 20: Training Progress Visualization (Local-Safe Version)\n",
    "print(\"üìä Starting training visualization...\")\n",
    "\n",
    "# Load training history with minimal overhead\n",
    "try:\n",
    "    history_path = get_platform_path('saved_data/training_history.pkl')\n",
    "    with open(history_path, 'rb') as f:\n",
    "        saved_history = pickle.load(f)\n",
    "    print(\"‚úÖ Training history loaded\")\n",
    "except:\n",
    "    saved_history = None\n",
    "    print(\"‚ö†Ô∏è No training history found\")\n",
    "\n",
    "# Display results in text format (guaranteed to work)\n",
    "if saved_history and saved_history.get('val_bleu'):\n",
    "    epochs = len(saved_history['val_bleu'])\n",
    "    \n",
    "    print(\"\\n\" + \"üéØ\" + \"=\"*60 + \"üéØ\")\n",
    "    print(\"          EMPATHETIC CHATBOT TRAINING RESULTS\")\n",
    "    print(\"üéØ\" + \"=\"*60 + \"üéØ\")\n",
    "    \n",
    "    print(f\"\\nüìà TRAINING OVERVIEW:\")\n",
    "    print(f\"   Total Epochs: {epochs}\")\n",
    "    print(f\"   Model Size: 47M parameters\")\n",
    "    print(f\"   Dataset: 64,591 dialogue pairs\")\n",
    "    \n",
    "    print(f\"\\nüèÜ FINAL PERFORMANCE:\")\n",
    "    final_bleu = saved_history['val_bleu'][-1]\n",
    "    final_rouge = saved_history['val_rouge_l'][-1]\n",
    "    final_chrf = saved_history['val_chrf'][-1]\n",
    "    final_perp = saved_history['val_perplexity'][-1]\n",
    "    \n",
    "    print(f\"   BLEU Score:    {final_bleu:.4f}\")\n",
    "    print(f\"   ROUGE-L Score: {final_rouge:.4f}\")\n",
    "    print(f\"   chrF Score:    {final_chrf:.2f}%\")\n",
    "    print(f\"   Perplexity:    {final_perp:.2f}\")\n",
    "    \n",
    "    print(f\"\\nüìä TRAINING PROGRESS:\")\n",
    "    print(\"   Epoch | Train Loss | Val Loss | BLEU   | ROUGE-L\")\n",
    "    print(\"   ------|------------|----------|--------|--------\")\n",
    "    \n",
    "    for i in range(min(epochs, 10)):  # Show max 10 epochs to avoid clutter\n",
    "        tl = saved_history.get('train_loss', [0])[i] if i < len(saved_history.get('train_loss', [])) else 0\n",
    "        vl = saved_history.get('val_loss', [0])[i] if i < len(saved_history.get('val_loss', [])) else 0\n",
    "        bleu = saved_history['val_bleu'][i]\n",
    "        rouge = saved_history['val_rouge_l'][i]\n",
    "        print(f\"   {i+1:5d} | {tl:10.4f} | {vl:8.4f} | {bleu:.4f} | {rouge:.4f}\")\n",
    "    \n",
    "    if epochs > 10:\n",
    "        print(f\"   ... ({epochs-10} more epochs)\")\n",
    "    \n",
    "    print(\"üéØ\" + \"=\"*60 + \"üéØ\")\n",
    "    \n",
    "    # Create simple ASCII chart for key metric\n",
    "    print(f\"\\nüìà BLEU Score Progress (ASCII Chart):\")\n",
    "    max_bleu = max(saved_history['val_bleu'])\n",
    "    min_bleu = min(saved_history['val_bleu'])\n",
    "    \n",
    "    for i, bleu in enumerate(saved_history['val_bleu'][:10], 1):  # Show first 10 epochs\n",
    "        # Normalize to 0-20 scale for ASCII bar\n",
    "        if max_bleu > min_bleu:\n",
    "            bar_length = int(((bleu - min_bleu) / (max_bleu - min_bleu)) * 20)\n",
    "        else:\n",
    "            bar_length = 10\n",
    "        bar = \"‚ñà\" * bar_length + \"‚ñë\" * (20 - bar_length)\n",
    "        print(f\"   Epoch {i:2d}: |{bar}| {bleu:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå No training history available\")\n",
    "    print(\"üí° Please run the training loop first (Section 17)\")\n",
    "    print(\"üìÅ Expected: saved_data/training_history.pkl\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training visualization completed!\")\n",
    "print(f\"üîí Safe for all platforms (no matplotlib dependencies)\")\n",
    "print(f\"üìä Results displayed in text format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02063a6",
   "metadata": {},
   "source": [
    "## Section 21: Human Evaluation Framework\n",
    "\n",
    "Framework for manual assessment of model outputs on Fluency, Relevance, and Adequacy (1-5 scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578204e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human Evaluation Framework\n",
    "print(\"üßë‚Äçüíº Human Evaluation Framework\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "EVALUATION CRITERIA (1-5 Scale):\n",
    "\n",
    "üó£Ô∏è FLUENCY (1-5):\n",
    "   1 = Completely ungrammatical, incomprehensible\n",
    "   2 = Mostly ungrammatical, hard to understand  \n",
    "   3 = Some grammatical errors, but understandable\n",
    "   4 = Mostly grammatical, minor errors\n",
    "   5 = Perfect grammar, natural flow\n",
    "\n",
    "üéØ RELEVANCE (1-5):\n",
    "   1 = Completely irrelevant to context\n",
    "   2 = Somewhat relevant but misses key points\n",
    "   3 = Moderately relevant, addresses some context\n",
    "   4 = Highly relevant, addresses most context\n",
    "   5 = Perfectly relevant, fully contextual\n",
    "\n",
    "üíù ADEQUACY (1-5):\n",
    "   1 = Completely inadequate empathetic response\n",
    "   2 = Minimal empathy, inappropriate tone\n",
    "   3 = Some empathy shown, neutral response\n",
    "   4 = Good empathetic understanding and response\n",
    "   5 = Excellent empathy, highly appropriate response\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# Generate evaluation samples\n",
    "evaluation_samples = []\n",
    "sample_conversations = test_cases[:3]  # Use first 3 test cases\n",
    "\n",
    "print(\"SAMPLE EVALUATION:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, conversation in enumerate(sample_conversations, 1):\n",
    "    print(f\"\\nüìã Sample {i}:\")\n",
    "    print(f\"Emotion: {conversation['emotion']}\")\n",
    "    print(f\"Situation: {conversation['situation']}\")\n",
    "    print(f\"Customer: {conversation['customer']}\")\n",
    "    \n",
    "    # Generate response\n",
    "    generated = generate_response(\n",
    "        trained_model,\n",
    "        conversation['emotion'], \n",
    "        conversation['situation'],\n",
    "        conversation['customer'],\n",
    "        model_vocab,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated Response: '{generated}'\")\n",
    "    print(f\"Ground Truth: '{conversation['ground_truth']}'\")\n",
    "    \n",
    "    # Create evaluation template\n",
    "    evaluation_sample = {\n",
    "        'sample_id': i,\n",
    "        'context': {\n",
    "            'emotion': conversation['emotion'],\n",
    "            'situation': conversation['situation'], \n",
    "            'customer': conversation['customer']\n",
    "        },\n",
    "        'generated_response': generated,\n",
    "        'ground_truth': conversation['ground_truth'],\n",
    "        'scores': {\n",
    "            'fluency': None,      # To be filled by human evaluator\n",
    "            'relevance': None,    # To be filled by human evaluator  \n",
    "            'adequacy': None      # To be filled by human evaluator\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    evaluation_samples.append(evaluation_sample)\n",
    "    \n",
    "    print(f\"\\nüìù EVALUATION TEMPLATE FOR SAMPLE {i}:\")\n",
    "    print(f\"   Fluency (1-5): _____\")\n",
    "    print(f\"   Relevance (1-5): _____\") \n",
    "    print(f\"   Adequacy (1-5): _____\")\n",
    "    print(f\"   Comments: ________________\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Save evaluation framework\n",
    "with open('saved_data/human_evaluation_samples.pkl', 'wb') as f:\n",
    "    pickle.dump(evaluation_samples, f)\n",
    "\n",
    "print(f\"\\n‚úÖ Human Evaluation Framework Setup Complete!\")\n",
    "print(f\"   üìÅ Evaluation samples saved to 'saved_data/human_evaluation_samples.pkl'\")\n",
    "print(f\"   üìä {len(evaluation_samples)} samples prepared for human evaluation\")\n",
    "print(f\"   üìã Manual scoring on 1-5 scale for Fluency, Relevance, Adequacy\")\n",
    "\n",
    "# Example scoring function for future use\n",
    "def calculate_human_scores(evaluation_results):\n",
    "    \"\"\"\n",
    "    Calculate average human evaluation scores.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_results: List of completed evaluation samples with scores\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with average scores\n",
    "    \"\"\"\n",
    "    if not evaluation_results:\n",
    "        return None\n",
    "        \n",
    "    fluency_scores = [sample['scores']['fluency'] for sample in evaluation_results \n",
    "                     if sample['scores']['fluency'] is not None]\n",
    "    relevance_scores = [sample['scores']['relevance'] for sample in evaluation_results \n",
    "                       if sample['scores']['relevance'] is not None]\n",
    "    adequacy_scores = [sample['scores']['adequacy'] for sample in evaluation_results \n",
    "                      if sample['scores']['adequacy'] is not None]\n",
    "    \n",
    "    return {\n",
    "        'avg_fluency': sum(fluency_scores) / len(fluency_scores) if fluency_scores else 0,\n",
    "        'avg_relevance': sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0,\n",
    "        'avg_adequacy': sum(adequacy_scores) / len(adequacy_scores) if adequacy_scores else 0,\n",
    "        'total_samples': len(evaluation_results)\n",
    "    }\n",
    "\n",
    "print(f\"\\nüìà Use calculate_human_scores() function to analyze completed evaluations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
